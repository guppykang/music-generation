{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model import *\n",
    "from dataloader import *\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['epochs'] = 20\n",
    "config['temperature'] = 0.7\n",
    "config['num_neurons'] = 100\n",
    "config['num_layers'] = 1\n",
    "config['input_dim'] = None\n",
    "config['learning_rate'] = 0.001\n",
    "config['saved_path'] = 'Saved_weights_baseline.pth'\n",
    "config['validation_loss_path'] = 'val_loss_baseline.out' \n",
    "config['training_loss_path'] = 'training_loss_baseline.out' \n",
    "config['early_stop_epoch'] = 3\n",
    "config['early_stopping'] = False\n",
    "config['max_song_length'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in saved\n",
      "dict_keys(['4', 'A', '<', 't', '\\t', '8', 'v', 'b', '@', '!', '^', 'K', ')', '9', 'o', 'z', 'y', 'V', 'Q', '3', 'u', '0', 'c', '\\\\', 'r', 'U', '+', 'p', ':', 'L', 'T', '6', '_', 'm', 'N', 'w', 'D', '>', 'i', ' ', '}', ',', 'X', 'g', 'I', 'k', 'a', '{', 'O', 'E', '*', 'x', '-', 'q', 'J', '1', '/', 'P', '.', 'Z', '7', 'd', '$', 'h', 'C', 'M', '~', 'n', '#', '?', '\"', '\\n', ']', '|', '&', 'F', \"'\", '[', 'W', 's', 'S', 'B', '2', 'e', 'G', 'j', 'l', '5', '=', 'f', 'H', 'R', 'Y', '(', '%'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = open('train.txt', 'r')\n",
    "train_songs = []\n",
    "\n",
    "val_file = open('val.txt', 'r')\n",
    "val_songs = []\n",
    "\n",
    "test_file = open('test.txt', 'r')\n",
    "test_songs = []\n",
    "\n",
    "\n",
    "if not os.path.exists('one_hot_decode.pkl') or not os.path.exists('one_hot_encode.pkl'):  \n",
    "    print('creating new')\n",
    "    all_characters = train_file.read()\n",
    "    unique_characters = list(set(all_characters))\n",
    "\n",
    "    print((unique_characters))\n",
    "\n",
    "\n",
    "    #create one hot encodings for each unique character in the alphabet of the training data\n",
    "    one_hot_dict_encode = {}\n",
    "    one_hot_dict_decode = {}\n",
    "    index = 0\n",
    "    for unique_character in unique_characters:\n",
    "        current_encoding = np.zeros(config['input_dim'])\n",
    "        current_encoding[index] = 1\n",
    "\n",
    "        one_hot_dict_encode[unique_character] = current_encoding\n",
    "        one_hot_dict_decode[tuple(current_encoding)] = unique_character\n",
    "\n",
    "        index += 1\n",
    "\n",
    "\n",
    "    #start token\n",
    "    current_encoding = np.zeros(config['input_dim'])\n",
    "    current_encoding[93] = 1\n",
    "\n",
    "    one_hot_dict_encode['$'] = current_encoding\n",
    "    one_hot_dict_decode[tuple(current_encoding)] = '$'\n",
    "\n",
    "    #end song token\n",
    "    current_encoding = np.zeros(config['input_dim'])\n",
    "    current_encoding[94] = 1\n",
    "\n",
    "    one_hot_dict_encode['%'] = current_encoding\n",
    "    one_hot_dict_decode[tuple(current_encoding)] = '%'\n",
    "    \n",
    "    f = open(\"one_hot_decode.pkl\",\"wb\")\n",
    "    pickle.dump(one_hot_dict_decode, f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(\"one_hot_encode.pkl\", \"wb\")\n",
    "    pickle.dump(one_hot_dict_encode, f)\n",
    "    f.close()\n",
    "else : \n",
    "    print('read in saved')\n",
    "    f = open(\"one_hot_decode.pkl\",\"rb\")\n",
    "    one_hot_dict_decode = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(\"one_hot_encode.pkl\", \"rb\")\n",
    "    one_hot_dict_encode = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "config['input_dim'] = len(one_hot_dict_decode)\n",
    "\n",
    "print(one_hot_dict_encode.keys())\n",
    "\n",
    "train_file = open('train.txt', 'r')\n",
    "train_songs = []\n",
    "\n",
    "val_file = open('val.txt', 'r')\n",
    "val_songs = []\n",
    "\n",
    "test_file = open('test.txt', 'r')\n",
    "test_songs = []\n",
    "\n",
    "\n",
    "#split the songs into their own strings, while including start and end tags\n",
    "song = []\n",
    "for line in train_file:\n",
    "    for character in line:\n",
    "        song.append(one_hot_dict_encode[character])\n",
    "\n",
    "        if character == '%':\n",
    "            song.append(one_hot_dict_encode['\\n'])\n",
    "            train_songs.append(song)\n",
    "            song = []\n",
    "    \n",
    "song = []\n",
    "for line in val_file:\n",
    "    for character in line:\n",
    "        song.append(one_hot_dict_encode[character])\n",
    "\n",
    "        if character == '%':\n",
    "            song.append(one_hot_dict_encode['\\n'])\n",
    "            val_songs.append(song)\n",
    "            song = []\n",
    "\n",
    "song = []\n",
    "for line in test_file:\n",
    "    for character in line:\n",
    "        song.append(one_hot_dict_encode[character])\n",
    "\n",
    "        if character == '%':\n",
    "            song.append(one_hot_dict_encode['\\n'])\n",
    "            test_songs.append(song)\n",
    "            song = []   \n",
    "            \n",
    "            \n",
    "len(one_hot_dict_decode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(config['input_dim'])\n",
    "a[73] = 1\n",
    "print(one_hot_dict_decode[tuple(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Lstm(\n",
       "   (lstm_layer): LSTM(95, 100, dropout=0.2)\n",
       "   (fc): Linear(in_features=100, out_features=95, bias=True)\n",
       " ), Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.001\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Lstm(config['input_dim'], config['num_neurons'], config['num_layers']).to(computing_device)\n",
    "\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "if not os.path.exists(config['saved_path']):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, config['saved_path'])\n",
    "    \n",
    "    np.savetxt(config['training_loss_path'], np.array([0,0]))\n",
    "    np.savetxt(config['validation_loss_path'], np.array([0,0]))\n",
    "    \n",
    "model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPPPPOCCCHHHHH 1\n",
      "epoch 1 with train error 1.0933928489685059\n",
      "epoch 1 with val error 1.6117006540298462\n",
      "EPPPPOCCCHHHHH 2\n",
      "epoch 2 with train error 1.0845848321914673\n",
      "epoch 2 with val error 1.6113781929016113\n",
      "EPPPPOCCCHHHHH 3\n",
      "epoch 3 with train error 1.073896884918213\n",
      "epoch 3 with val error 1.6332435607910156\n",
      "EPPPPOCCCHHHHH 4\n",
      "epoch 4 with train error 1.0691072940826416\n",
      "epoch 4 with val error 1.607514500617981\n",
      "EPPPPOCCCHHHHH 5\n",
      "epoch 5 with train error 1.05618417263031\n",
      "epoch 5 with val error 1.6394963264465332\n",
      "EPPPPOCCCHHHHH 6\n",
      "epoch 6 with train error 1.0503875017166138\n",
      "epoch 6 with val error 1.6211785078048706\n",
      "EPPPPOCCCHHHHH 7\n",
      "epoch 7 with train error 1.0453078746795654\n",
      "epoch 7 with val error 1.6021947860717773\n",
      "EPPPPOCCCHHHHH 8\n",
      "epoch 8 with train error 1.0355521440505981\n",
      "epoch 8 with val error 1.6222515106201172\n",
      "EPPPPOCCCHHHHH 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-3ea8e296efeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m#forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/music-generation/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mis_size_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for early stopping\n",
    "num_times_incraesed = 0\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "#restore the model's weights \n",
    "checkpoint = torch.load(config['saved_path'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print('EPPPPOCCCHHHHH ' + str(epoch + 1))\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    #for early stopping\n",
    "    old_net_weights = model.state_dict()\n",
    "    old_optimizer = optimizer.state_dict()\n",
    "    \n",
    "    #metrics for train and val loss\n",
    "    training_losses = np.genfromtxt(config['training_loss_path'])\n",
    "    validation_losses = np.genfromtxt(config['validation_loss_path'])\n",
    "\n",
    "    #Shuffle the songs \n",
    "    random.shuffle(train_songs)\n",
    "    \n",
    "    #train\n",
    "    model.train()\n",
    "    for song in train_songs:\n",
    "\n",
    "        #set states to 0 at the beginning of each song\n",
    "        hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "        hidden_state = hidden_state.float()\n",
    "        cell_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "        cell_state = cell_state.float()\n",
    "        hidden = (hidden_state, cell_state)\n",
    "\n",
    "        \n",
    "        #encode the characters to their respective one hot encoding\n",
    "        encoded_inputs = song\n",
    "        \n",
    "        song_loss = 0\n",
    "        #train\n",
    "        num_minibatches = math.ceil(len(encoded_inputs)/100)\n",
    "        for i in range(num_minibatches):\n",
    "            model.zero_grad()\n",
    "\n",
    "            chunk, targets = MyDataset(encoded_inputs)[i]\n",
    "           \n",
    "            \n",
    "            \n",
    "            if len(chunk) == 0:\n",
    "                break\n",
    "            \n",
    "            #to computing device\n",
    "            chunk = chunk.to(computing_device)\n",
    "            targets = targets.to(computing_device)\n",
    "            \n",
    "            #forward\n",
    "            output, hidden = model(chunk, hidden)\n",
    "            \n",
    "\n",
    "            \n",
    "            #loss\n",
    "            targets = targets.argmax(dim=1)\n",
    "            loss = criterion(output, targets)\n",
    "            song_loss += loss\n",
    "            \n",
    "            #backprop\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_loss += song_loss/num_minibatches\n",
    "      \n",
    "    #calculate training loss\n",
    "    average_epoch_loss = total_loss/len(train_songs)\n",
    "    print('epoch ' + str(epoch + 1) + ' with train error '+ str(average_epoch_loss.cpu().item()))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    total_loss = 0\n",
    "    #validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for song in val_songs:\n",
    "            song_loss = 0\n",
    "            \n",
    "            #set states to 0 at the beginning of each song\n",
    "            hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "            hidden_state = hidden_state.float()\n",
    "            cell_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "            cell_state = cell_state.float()\n",
    "            hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "            #encode the characters to their respective one hot encoding\n",
    "            encoded_inputs = song\n",
    "\n",
    "\n",
    "            #val\n",
    "            num_minibatches = math.ceil(len(encoded_inputs)/100)\n",
    "            for i in range(num_minibatches):\n",
    "\n",
    "                chunk, targets = MyDataset(encoded_inputs)[i]\n",
    "                \n",
    "\n",
    "                if len(chunk) == 0:\n",
    "                    break\n",
    "\n",
    "                #to computing device\n",
    "                chunk = chunk.to(computing_device)\n",
    "                targets = targets.to(computing_device)\n",
    "\n",
    "                #forward\n",
    "                output, hidden = model(chunk, hidden)\n",
    "\n",
    "                targets = targets.argmax(dim=1)\n",
    "                \n",
    "#                 print('hi')\n",
    "#                 print(output.squeeze().argmax(dim=1))\n",
    "\n",
    "#                 print('mom')\n",
    "#                 print(targets)\n",
    "                loss = criterion(output, targets)\n",
    "                song_loss += loss\n",
    "                \n",
    "            total_loss += song_loss/num_minibatches\n",
    "\n",
    "                \n",
    "    #calculate training loss\n",
    "    average_val_epoch_loss = total_loss/len(val_songs)\n",
    "    print('epoch ' + str(epoch + 1) + ' with val error '+ str(average_val_epoch_loss.cpu().item()))\n",
    "    \n",
    "    \n",
    "    if config['early_stopping']:\n",
    "        #early stopping\n",
    "        if average_val_epoch_loss > prev_val_loss:\n",
    "            print('keeping old weights')\n",
    "            num_times_incraesed += 1\n",
    "            model.load_state_dict(old_net_weights)\n",
    "            optimizer.load_state_dict(old_optimizer)\n",
    "        else : \n",
    "            print('val is less than previous')\n",
    "            num_times_incraesed = 0\n",
    "            prev_val_loss = average_val_epoch_loss\n",
    "\n",
    "        if num_times_incraesed >= config['early_stop_epoch']:\n",
    "            print('early stopping triggered')\n",
    "            break       \n",
    "            \n",
    "            \n",
    "    #save model and loss\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, config['saved_path'])\n",
    "    \n",
    "    np.savetxt(config['training_loss_path'], np.append(training_losses, average_epoch_loss.cpu().item()))\n",
    "    np.savetxt(config['validation_loss_path'], np.append(validation_losses, average_val_epoch_loss.cpu().item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RNN(\n",
       "   (rnn): RNN(95, 100, dropout=0.2)\n",
       "   (fc): Linear(in_features=100, out_features=95, bias=True)\n",
       " ), Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.001\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = RNN(config['input_dim'], config['num_neurons'], config['num_layers']).to(computing_device)\n",
    "\n",
    "    \n",
    "rnn_criterion = nn.CrossEntropyLoss()\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "if not os.path.exists(config['saved_path'] + '_rnn'):\n",
    "    torch.save({\n",
    "        'model_state_dict': rnn_model.state_dict(),\n",
    "        'optimizer_state_dict': rnn_optimizer.state_dict(),\n",
    "    }, config['saved_path']  + '_rnn')\n",
    "    \n",
    "    np.savetxt(config['training_loss_path'] + '_rnn', np.array([0,0]))\n",
    "    np.savetxt(config['validation_loss_path'] + '_rnn', np.array([0,0]))\n",
    "    \n",
    "rnn_model, rnn_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPPPPOCCCHHHHH 1\n",
      "epoch 1 with train error 2.553476095199585\n",
      "epoch 1 with val error 2.4413230419158936\n",
      "EPPPPOCCCHHHHH 2\n",
      "epoch 2 with train error 1.9926903247833252\n",
      "epoch 2 with val error 2.132091522216797\n",
      "EPPPPOCCCHHHHH 3\n",
      "epoch 3 with train error 1.852763295173645\n",
      "epoch 3 with val error 2.049252510070801\n",
      "EPPPPOCCCHHHHH 4\n",
      "epoch 4 with train error 1.7767468690872192\n",
      "epoch 4 with val error 1.9801504611968994\n",
      "EPPPPOCCCHHHHH 5\n",
      "epoch 5 with train error 1.7135109901428223\n",
      "epoch 5 with val error 2.0715765953063965\n",
      "EPPPPOCCCHHHHH 6\n",
      "epoch 6 with train error 1.6840397119522095\n",
      "epoch 6 with val error 1.9413748979568481\n",
      "EPPPPOCCCHHHHH 7\n",
      "epoch 7 with train error 1.6453713178634644\n",
      "epoch 7 with val error 1.957262635231018\n",
      "EPPPPOCCCHHHHH 8\n",
      "epoch 8 with train error 1.6245723962783813\n",
      "epoch 8 with val error 1.9191571474075317\n",
      "EPPPPOCCCHHHHH 9\n",
      "epoch 9 with train error 1.6055363416671753\n",
      "epoch 9 with val error 1.8921295404434204\n",
      "EPPPPOCCCHHHHH 10\n",
      "epoch 10 with train error 1.588417410850525\n",
      "epoch 10 with val error 1.9744529724121094\n",
      "EPPPPOCCCHHHHH 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-55fd0f50d48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m#backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for early stopping\n",
    "num_times_incraesed = 0\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "#restore the model's weights \n",
    "checkpoint = torch.load(config['saved_path'] + '_rnn')\n",
    "rnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "rnn_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print('EPPPPOCCCHHHHH ' + str(epoch + 1))\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    #for early stopping\n",
    "    old_net_weights = rnn_model.state_dict()\n",
    "    old_optimizer = rnn_optimizer.state_dict()\n",
    "    \n",
    "    #metrics for train and val loss\n",
    "    training_losses = np.genfromtxt(config['training_loss_path'] + '_rnn')\n",
    "    validation_losses = np.genfromtxt(config['validation_loss_path'] + '_rnn')\n",
    "\n",
    "    #Shuffle the songs \n",
    "    random.shuffle(train_songs)\n",
    "    \n",
    "    #train\n",
    "    rnn_model.train()\n",
    "    for song in train_songs:\n",
    "\n",
    "        #set states to 0 at the beginning of each song\n",
    "        hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "        hidden_state = hidden_state.float()\n",
    "        hidden = hidden_state\n",
    "\n",
    "        \n",
    "        #encode the characters to their respective one hot encoding\n",
    "        encoded_inputs = song\n",
    "        \n",
    "        song_loss = 0\n",
    "        #train\n",
    "        num_minibatches = math.ceil(len(encoded_inputs)/100)\n",
    "        for i in range(num_minibatches):\n",
    "            rnn_model.zero_grad()\n",
    "\n",
    "            chunk, targets = MyDataset(encoded_inputs)[i]\n",
    "           \n",
    "            \n",
    "            \n",
    "            if len(chunk) == 0:\n",
    "                break\n",
    "            \n",
    "            #to computing device\n",
    "            chunk = chunk.to(computing_device)\n",
    "            targets = targets.to(computing_device)\n",
    "            \n",
    "            #forward\n",
    "            output, hidden = rnn_model(chunk, hidden)\n",
    "            \n",
    "\n",
    "            \n",
    "            #loss\n",
    "            targets = targets.argmax(dim=1)\n",
    "            loss = rnn_criterion(output, targets)\n",
    "            song_loss += loss\n",
    "            \n",
    "            #backprop\n",
    "            loss.backward(retain_graph=True)\n",
    "            rnn_optimizer.step()\n",
    "            \n",
    "        total_loss += song_loss/num_minibatches\n",
    "      \n",
    "    #calculate training loss\n",
    "    average_epoch_loss = total_loss/len(train_songs)\n",
    "    print('epoch ' + str(epoch + 1) + ' with train error '+ str(average_epoch_loss.cpu().item()))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    total_loss = 0\n",
    "    #validation\n",
    "    rnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for song in val_songs:\n",
    "            song_loss = 0\n",
    "            \n",
    "            #set states to 0 at the beginning of each song\n",
    "            hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "            hidden_state = hidden_state.float()\n",
    "            hidden = hidden_state\n",
    "\n",
    "\n",
    "            #encode the characters to their respective one hot encoding\n",
    "            encoded_inputs = song\n",
    "\n",
    "\n",
    "            #val\n",
    "            num_minibatches = math.ceil(len(encoded_inputs)/100)\n",
    "            for i in range(num_minibatches):\n",
    "\n",
    "                chunk, targets = MyDataset(encoded_inputs)[i]\n",
    "                \n",
    "\n",
    "                if len(chunk) == 0:\n",
    "                    break\n",
    "\n",
    "                #to computing device\n",
    "                chunk = chunk.to(computing_device)\n",
    "                targets = targets.to(computing_device)\n",
    "\n",
    "                #forward\n",
    "                output, hidden = rnn_model(chunk, hidden)\n",
    "\n",
    "                targets = targets.argmax(dim=1)\n",
    "                \n",
    "#                 print('hi')\n",
    "#                 print(output.squeeze().argmax(dim=1))\n",
    "\n",
    "#                 print('mom')\n",
    "#                 print(targets)\n",
    "                loss = rnn_criterion(output, targets)\n",
    "                song_loss += loss\n",
    "                \n",
    "            total_loss += song_loss/num_minibatches\n",
    "\n",
    "                \n",
    "    #calculate training loss\n",
    "    average_val_epoch_loss = total_loss/len(val_songs)\n",
    "    print('epoch ' + str(epoch + 1) + ' with val error '+ str(average_val_epoch_loss.cpu().item()))\n",
    "    \n",
    "    \n",
    "    if config['early_stopping']:\n",
    "        #early stopping\n",
    "        if average_val_epoch_loss > prev_val_loss:\n",
    "            print('keeping old weights')\n",
    "            num_times_incraesed += 1\n",
    "            rnn_model.load_state_dict(old_net_weights)\n",
    "            rnn_optimizer.load_state_dict(old_optimizer)\n",
    "        else : \n",
    "            print('val is less than previous')\n",
    "            num_times_incraesed = 0\n",
    "            prev_val_loss = average_val_epoch_loss\n",
    "\n",
    "        if num_times_incraesed >= config['early_stop_epoch']:\n",
    "            print('early stopping triggered')\n",
    "            break       \n",
    "            \n",
    "            \n",
    "    #save model and loss\n",
    "    torch.save({\n",
    "        'model_state_dict': rnn_model.state_dict(),\n",
    "        'optimizer_state_dict': rnn_optimizer.state_dict(),\n",
    "    }, config['saved_path'] + '_rnn')\n",
    "    \n",
    "    np.savetxt(config['training_loss_path'] + '_rnn', np.append(training_losses, average_epoch_loss.cpu().item()))\n",
    "    np.savetxt(config['validation_loss_path'] + '_rnn', np.append(validation_losses, average_val_epoch_loss.cpu().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2623105  1.93900692 1.86721361 1.73471773 1.70964777 1.67382395\n",
      " 1.66307199 1.62063646 1.62720847 1.63149607 1.60833895 1.6112864\n",
      " 1.5962069  1.58226824 1.59238398 1.61996806 1.59337294 1.61170065\n",
      " 1.61137819 1.63324356 1.6075145  1.63949633 1.62117851 1.60219479\n",
      " 1.62225151]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f61a49dacc0>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = np.genfromtxt(config['training_loss_path'])[2:]\n",
    "validation_losses = np.genfromtxt(config['validation_loss_path'])[2:]\n",
    "print(validation_losses)\n",
    "\n",
    "x_axis = []\n",
    "for i in range(len(training_losses)):\n",
    "    x_axis.append(i)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_axis, training_losses)\n",
    "plt.plot(x_axis, validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature 0.7 seems to work the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":1\n",
      "T:John McBway\n",
      "O:France\n",
      "A:Provence\n",
      "R:Marche\n",
      "Z:Transcrit et/ou corrig? par Michel BELLON - 2005-07-26\n",
      "Z:Pour toute observation mailto:galouvielle@free.fr\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:Gm\n",
      "GA BG | G2 GB | c2 c2 | d2 d2 | d2 dd | d2 d2 | d2 d2 | d2 d2 | d2 cB |\n",
      "A2 A2 | G2 GG | A2 A2 | A2 A2 | G3 :|\n",
      "|: d | e2 e2 | d2 d2 | d2 dd | e2 ed | c2 A2 | B2 Bz | G2 G2 | G2 A2 | B2 B2 | c2 A2 | G2 E2 | G2 E2 |\n",
      "A>B AG | A2 A2 | G4 | A4 | G>A B2 | G2 G2 | G2 G2 | G2 A2 | G2 G2 | A2 A2 | \n",
      "G2 GA | B2 A2 | G2 A2 | G4 ||\n",
      "P:Intro\n",
      "a | e2 de | d2 d2 | d2 d2 | d2 ed | c2 A2 | B2 c2 | B2 AG | A2 A2 | G2 AB | c2 c2 | A2 BA | G2 G2 :|\n",
      "%\n"
     ]
    }
   ],
   "source": [
    "#get first initial input\n",
    "output = torch.from_numpy(np.array(one_hot_dict_encode['X'])).float()[None,None,:].to(computing_device) \n",
    "                \n",
    "#init hidden state\n",
    "hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "hidden_state = hidden_state.float()\n",
    "cell_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "cell_state = cell_state.float()\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "new_song = []\n",
    "\n",
    "#generate music\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    while output.squeeze().argmax().item() != 94 : \n",
    "        output, hidden = model(output, hidden)\n",
    "        \n",
    "        softmax = F.softmax(output/config['temperature'])\n",
    "        \n",
    "        start = 0\n",
    "        partitions = []\n",
    "        \n",
    "        for i in softmax[0]:\n",
    "            partitions.append((start, start + i.item()))\n",
    "            start = start + i.item()\n",
    "        \n",
    "        roll = random.uniform(0, partitions[-1][1])\n",
    "        \n",
    "        \n",
    "        guess = 0\n",
    "        for partition in partitions:\n",
    "            if roll >= partition[0] and roll < partition[1]:\n",
    "                break\n",
    "            else : \n",
    "                guess += 1\n",
    "        \n",
    "\n",
    "        next_input = np.zeros(config['input_dim'])\n",
    "        next_input[guess] = 1\n",
    "        \n",
    "        new_song.append(next_input)\n",
    "        \n",
    "        \n",
    "        output = torch.from_numpy(next_input).float()[None, None, :].to(computing_device)\n",
    "        \n",
    "\n",
    "decoded_string = \"\"\n",
    "for encoding in new_song:\n",
    "#     print(one_hot_dict_decode[tuple(encoding)], end =\" \")\n",
    "    decoded_string += one_hot_dict_decode[tuple(encoding)]\n",
    "print(decoded_string)\n",
    "            \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
