{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model import *\n",
    "from dataloader import *\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['epochs'] = 100\n",
    "config['temperature'] = 1\n",
    "config['num_neurons'] = 100\n",
    "config['num_layers'] = 1\n",
    "config['input_dim'] = None\n",
    "config['learning_rate'] = 0.001\n",
    "config['saved_path'] = 'Saved_weights_baseline.pth'\n",
    "config['validation_loss_path'] = 'val_loss_baseline.out' \n",
    "config['training_loss_path'] = 'training_loss_baseline.out' \n",
    "config['early_stop_epoch'] = 3\n",
    "config['early_stopping'] = False\n",
    "config['max_song_length'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is supported\n"
     ]
    }
   ],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in saved\n",
      "dict_keys(['4', 'A', '<', 't', '\\t', '8', 'v', 'b', '@', '!', '^', 'K', ')', '9', 'o', 'z', 'y', 'V', 'Q', '3', 'u', '0', 'c', '\\\\', 'r', 'U', '+', 'p', ':', 'L', 'T', '6', '_', 'm', 'N', 'w', 'D', '>', 'i', ' ', '}', ',', 'X', 'g', 'I', 'k', 'a', '{', 'O', 'E', '*', 'x', '-', 'q', 'J', '1', '/', 'P', '.', 'Z', '7', 'd', '$', 'h', 'C', 'M', '~', 'n', '#', '?', '\"', '\\n', ']', '|', '&', 'F', \"'\", '[', 'W', 's', 'S', 'B', '2', 'e', 'G', 'j', 'l', '5', '=', 'f', 'H', 'R', 'Y', '(', '%'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(95, 95)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file = open('train.txt', 'r')\n",
    "train_songs = []\n",
    "\n",
    "val_file = open('val.txt', 'r')\n",
    "val_songs = []\n",
    "\n",
    "test_file = open('test.txt', 'r')\n",
    "test_songs = []\n",
    "\n",
    "\n",
    "if not os.path.exists('one_hot_decode.pkl') or not os.path.exists('one_hot_encode.pkl'):  \n",
    "    print('creating new')\n",
    "    all_characters = train_file.read()\n",
    "    unique_characters = list(set(all_characters))\n",
    "\n",
    "    print((unique_characters))\n",
    "\n",
    "\n",
    "    #create one hot encodings for each unique character in the alphabet of the training data\n",
    "    one_hot_dict_encode = {}\n",
    "    one_hot_dict_decode = {}\n",
    "    index = 0\n",
    "    for unique_character in unique_characters:\n",
    "        current_encoding = np.zeros(config['input_dim'])\n",
    "        current_encoding[index] = 1\n",
    "\n",
    "        one_hot_dict_encode[unique_character] = current_encoding\n",
    "        one_hot_dict_decode[tuple(current_encoding)] = unique_character\n",
    "\n",
    "        index += 1\n",
    "\n",
    "\n",
    "    #start token\n",
    "    current_encoding = np.zeros(config['input_dim'])\n",
    "    current_encoding[93] = 1\n",
    "\n",
    "    one_hot_dict_encode['$'] = current_encoding\n",
    "    one_hot_dict_decode[tuple(current_encoding)] = '$'\n",
    "\n",
    "    #end song token\n",
    "    current_encoding = np.zeros(config['input_dim'])\n",
    "    current_encoding[94] = 1\n",
    "\n",
    "    one_hot_dict_encode['%'] = current_encoding\n",
    "    one_hot_dict_decode[tuple(current_encoding)] = '%'\n",
    "    \n",
    "    f = open(\"one_hot_decode.pkl\",\"wb\")\n",
    "    pickle.dump(one_hot_dict_decode, f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(\"one_hot_encode.pkl\", \"wb\")\n",
    "    pickle.dump(one_hot_dict_encode, f)\n",
    "    f.close()\n",
    "else : \n",
    "    print('read in saved')\n",
    "    f = open(\"one_hot_decode.pkl\",\"rb\")\n",
    "    one_hot_dict_decode = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(\"one_hot_encode.pkl\", \"rb\")\n",
    "    one_hot_dict_encode = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "config['input_dim'] = len(one_hot_dict_decode)\n",
    "\n",
    "print(one_hot_dict_encode.keys())\n",
    "\n",
    "train_file = open('train.txt', 'r')\n",
    "train_songs = []\n",
    "\n",
    "val_file = open('val.txt', 'r')\n",
    "val_songs = []\n",
    "\n",
    "test_file = open('test.txt', 'r')\n",
    "test_songs = []\n",
    "\n",
    "\n",
    "#split the songs into their own strings, while including start and end tags\n",
    "song = []\n",
    "for line in train_file:\n",
    "    for character in line:\n",
    "        song.append(one_hot_dict_encode[character])\n",
    "\n",
    "        if character == '%':\n",
    "            song.append(one_hot_dict_encode['\\n'])\n",
    "            train_songs.append(song)\n",
    "            song = []\n",
    "    \n",
    "song = []\n",
    "for line in val_file:\n",
    "    for character in line:\n",
    "        song.append(one_hot_dict_encode[character])\n",
    "\n",
    "        if character == '%':\n",
    "            song.append(one_hot_dict_encode['\\n'])\n",
    "            val_songs.append(song)\n",
    "            song = []\n",
    "\n",
    "song = []\n",
    "for line in test_file:\n",
    "    for character in line:\n",
    "        song.append(one_hot_dict_encode[character])\n",
    "\n",
    "        if character == '%':\n",
    "            song.append(one_hot_dict_encode['\\n'])\n",
    "            test_songs.append(song)\n",
    "            song = []   \n",
    "            \n",
    "            \n",
    "len(unique_characters), len(one_hot_dict_decode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(config['input_dim'])\n",
    "a[71] = 1\n",
    "print(one_hot_dict_decode[tuple(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Lstm(\n",
       "   (lstm_layer): LSTM(95, 100)\n",
       "   (fc): Linear(in_features=100, out_features=95, bias=True)\n",
       " ), Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.001\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Lstm(config['input_dim'], config['num_neurons'], config['num_layers']).to(computing_device)\n",
    "\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "if not os.path.exists(config['saved_path']):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, config['saved_path'])\n",
    "    \n",
    "    np.savetxt(config['training_loss_path'], np.array([0,0]))\n",
    "    np.savetxt(config['validation_loss_path'], np.array([0,0]))\n",
    "    \n",
    "model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPPPPOCCCHHHHH 1\n",
      "epoch 1 with train error 2.7620954513549805\n",
      "epoch 1 with val error 2.448500394821167\n",
      "EPPPPOCCCHHHHH 2\n",
      "epoch 2 with train error 1.9228054285049438\n",
      "epoch 2 with val error 1.9886987209320068\n",
      "EPPPPOCCCHHHHH 3\n",
      "epoch 3 with train error 1.651258111000061\n",
      "epoch 3 with val error 1.813874363899231\n",
      "EPPPPOCCCHHHHH 4\n",
      "epoch 4 with train error 1.5165067911148071\n",
      "epoch 4 with val error 1.7712212800979614\n",
      "EPPPPOCCCHHHHH 5\n",
      "epoch 5 with train error 1.4359749555587769\n",
      "epoch 5 with val error 1.738441824913025\n",
      "EPPPPOCCCHHHHH 6\n",
      "epoch 6 with train error 1.375138282775879\n",
      "epoch 6 with val error 1.6752171516418457\n",
      "EPPPPOCCCHHHHH 7\n",
      "epoch 7 with train error 1.329209804534912\n",
      "epoch 7 with val error 1.650712251663208\n",
      "EPPPPOCCCHHHHH 8\n",
      "epoch 8 with train error 1.291443109512329\n",
      "epoch 8 with val error 1.6475664377212524\n",
      "EPPPPOCCCHHHHH 9\n",
      "epoch 9 with train error 1.2604888677597046\n",
      "epoch 9 with val error 1.6252671480178833\n",
      "EPPPPOCCCHHHHH 10\n",
      "epoch 10 with train error 1.2332487106323242\n",
      "epoch 10 with val error 1.6087044477462769\n",
      "EPPPPOCCCHHHHH 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3ea8e296efeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msong_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_minibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for early stopping\n",
    "num_times_incraesed = 0\n",
    "prev_val_loss = float(\"inf\")\n",
    "\n",
    "\n",
    "#restore the model's weights \n",
    "checkpoint = torch.load(config['saved_path'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print('EPPPPOCCCHHHHH ' + str(epoch + 1))\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    #for early stopping\n",
    "    old_net_weights = model.state_dict()\n",
    "    old_optimizer = optimizer.state_dict()\n",
    "    \n",
    "    #metrics for train and val loss\n",
    "    training_losses = np.genfromtxt(config['training_loss_path'])\n",
    "    validation_losses = np.genfromtxt(config['validation_loss_path'])\n",
    "\n",
    "    #Shuffle the songs \n",
    "    random.shuffle(train_songs)\n",
    "    \n",
    "    #train\n",
    "    model.train()\n",
    "    for song in train_songs:\n",
    "\n",
    "        #set states to 0 at the beginning of each song\n",
    "        hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "        hidden_state = hidden_state.float()\n",
    "        cell_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "        cell_state = cell_state.float()\n",
    "        hidden = (hidden_state, cell_state)\n",
    "\n",
    "        \n",
    "        #encode the characters to their respective one hot encoding\n",
    "        encoded_inputs = song\n",
    "        \n",
    "        song_loss = 0\n",
    "        #train\n",
    "        num_minibatches = math.ceil(len(encoded_inputs)/100)\n",
    "        for i in range(num_minibatches):\n",
    "            model.zero_grad()\n",
    "\n",
    "            chunk, targets = MyDataset(encoded_inputs)[i]\n",
    "           \n",
    "            \n",
    "            \n",
    "            if len(chunk) == 0:\n",
    "                break\n",
    "            \n",
    "            #to computing device\n",
    "            chunk = chunk.to(computing_device)\n",
    "            targets = targets.to(computing_device)\n",
    "            \n",
    "            #forward\n",
    "            output, hidden = model(chunk, hidden)\n",
    "            \n",
    "\n",
    "            \n",
    "            #loss\n",
    "            targets = targets.argmax(dim=1)\n",
    "            loss = criterion(output, targets)\n",
    "            song_loss += loss\n",
    "            \n",
    "            #backprop\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_loss += song_loss/num_minibatches\n",
    "      \n",
    "    #calculate training loss\n",
    "    average_epoch_loss = total_loss/len(train_songs)\n",
    "    print('epoch ' + str(epoch + 1) + ' with train error '+ str(average_epoch_loss.cpu().item()))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    total_loss = 0\n",
    "    #validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for song in val_songs:\n",
    "            song_loss = 0\n",
    "            \n",
    "            #set states to 0 at the beginning of each song\n",
    "            hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "            hidden_state = hidden_state.float()\n",
    "            cell_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "            cell_state = cell_state.float()\n",
    "            hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "            #encode the characters to their respective one hot encoding\n",
    "            encoded_inputs = song\n",
    "\n",
    "\n",
    "            #val\n",
    "            num_minibatches = math.ceil(len(encoded_inputs)/100)\n",
    "            for i in range(num_minibatches):\n",
    "\n",
    "                chunk, targets = MyDataset(encoded_inputs)[i]\n",
    "                \n",
    "\n",
    "                if len(chunk) == 0:\n",
    "                    break\n",
    "\n",
    "                #to computing device\n",
    "                chunk = chunk.to(computing_device)\n",
    "                targets = targets.to(computing_device)\n",
    "\n",
    "                #forward\n",
    "                output, hidden = model(chunk, hidden)\n",
    "\n",
    "                targets = targets.argmax(dim=1)\n",
    "                \n",
    "#                 print('hi')\n",
    "#                 print(output.squeeze().argmax(dim=1))\n",
    "\n",
    "#                 print('mom')\n",
    "#                 print(targets)\n",
    "                loss = criterion(output, targets)\n",
    "                song_loss += loss\n",
    "                \n",
    "            total_loss += song_loss/num_minibatches\n",
    "\n",
    "                \n",
    "    #calculate training loss\n",
    "    average_val_epoch_loss = total_loss/len(val_songs)\n",
    "    print('epoch ' + str(epoch + 1) + ' with val error '+ str(average_val_epoch_loss.cpu().item()))\n",
    "    \n",
    "    \n",
    "    if config['early_stopping']:\n",
    "        #early stopping\n",
    "        if average_val_epoch_loss > prev_val_loss:\n",
    "            print('keeping old weights')\n",
    "            num_times_incraesed += 1\n",
    "            model.load_state_dict(old_net_weights)\n",
    "            optimizer.load_state_dict(old_optimizer)\n",
    "        else : \n",
    "            print('val is less than previous')\n",
    "            num_times_incraesed = 0\n",
    "            prev_val_loss = average_val_epoch_loss\n",
    "\n",
    "        if num_times_incraesed >= config['early_stop_epoch']:\n",
    "            print('early stopping triggered')\n",
    "            break       \n",
    "            \n",
    "            \n",
    "    #save model and loss\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, config['saved_path'])\n",
    "    \n",
    "    np.savetxt(config['training_loss_path'], np.append(training_losses, average_epoch_loss.cpu().item()))\n",
    "    np.savetxt(config['validation_loss_path'], np.append(validation_losses, average_val_epoch_loss.cpu().item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.44850039 1.98869872 1.81387436 1.77122128 1.73844182 1.67521715\n",
      " 1.65071225 1.64756644 1.62526715 1.60870445]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f462972a160>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = np.genfromtxt(config['training_loss_path'])[2:]\n",
    "validation_losses = np.genfromtxt(config['validation_loss_path'])[2:]\n",
    "print(validation_losses)\n",
    "\n",
    "x_axis = []\n",
    "for i in range(len(training_losses)):\n",
    "    x_axis.append(i)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_axis, training_losses)\n",
    "plt.plot(x_axis, validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "c\n",
      "6\n",
      "t\n",
      "h\n",
      "n\n",
      " \n",
      ":\n",
      "|\n",
      ":\n",
      "\n",
      "\n",
      "D\n",
      ":\n",
      "D\n",
      "c\n",
      " \n",
      "A\n",
      "A\n",
      "/\n",
      "r\n",
      "a\n",
      "r\n",
      "i\n",
      "s\n",
      "e\n",
      "l\n",
      "-\n",
      "s\n",
      "0\n",
      " \n",
      "p\n",
      " \n",
      "K\n",
      "a\n",
      "m\n",
      "\n",
      "\n",
      "%\n",
      "\n",
      "\n",
      "X\n",
      ":\n",
      "2\n",
      "7\n",
      "\n",
      "\n",
      "T\n",
      ":\n",
      "D\n",
      "a\n",
      "r\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "F\n",
      "a\n",
      "g\n",
      "y\n",
      "'\n",
      "s\n",
      "\n",
      "\n",
      "O\n",
      ":\n",
      "F\n",
      "r\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      "\n",
      "\n",
      "A\n",
      ":\n",
      "P\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      "\n",
      "\n",
      "C\n",
      ":\n",
      "N\n",
      "i\n",
      "m\n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "a\n",
      "m\n",
      "p\n",
      "e\n",
      "s\n",
      " \n",
      "W\n",
      "i\n",
      "l\n",
      "e\n",
      " \n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      "n\n",
      "\n",
      "\n",
      "T\n",
      ":\n",
      "A\n",
      "i\n",
      "t\n",
      " \n",
      "u\n",
      "n\n",
      " \n",
      "b\n",
      "r\n",
      "e\n",
      "y\n",
      " \n",
      "B\n",
      "a\n",
      "n\n",
      "d\n",
      "\n",
      "\n",
      "W\n",
      ":\n",
      "I\n",
      "n\n",
      " \n",
      "w\n",
      "a\n",
      "d\n",
      " \n",
      "r\n",
      "o\n",
      "d\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "B\n",
      "r\n",
      "a\n",
      "n\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "1\n",
      "9\n",
      "9\n",
      "0\n",
      "\n",
      "\n",
      "Z\n",
      ":\n",
      "i\n",
      "d\n",
      ":\n",
      "h\n",
      "n\n",
      "-\n",
      "b\n",
      "a\n",
      "r\n",
      "n\n",
      "a\n",
      "\n",
      "\n",
      "O\n",
      ":\n",
      "F\n",
      "r\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      "\n",
      "\n",
      "S\n",
      ":\n",
      "C\n",
      "h\n",
      "a\n",
      "m\n",
      "e\n",
      "l\n",
      "a\n",
      "n\n",
      "t\n",
      "\n",
      "\n",
      "N\n",
      ":\n",
      "\n",
      "\n",
      "Z\n",
      ":\n",
      "P\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      "u\n",
      "t\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "v\n",
      "e\n",
      " \n",
      "~\n",
      "a\n",
      "r\n",
      "c\n",
      "t\n",
      "i\n",
      "e\n",
      "l\n",
      "e\n",
      "@\n",
      "l\n",
      "i\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "S\n",
      ":\n",
      "P\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "e\n",
      " \n",
      "b\n",
      "o\n",
      "r\n",
      " \n",
      "p\n",
      "i\n",
      "p\n",
      "e\n",
      " \n",
      "S\n",
      "e\n",
      "r\n",
      "o\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "$\n",
      "1\n",
      "6\n",
      "8\n",
      "0\n",
      "s\n",
      "-\n",
      "1\n",
      "5\n",
      "\n",
      "\n",
      "Z\n",
      ":\n",
      "i\n",
      "d\n",
      ":\n",
      "h\n",
      "n\n",
      "-\n",
      "m\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "-\n",
      "8\n",
      "\n",
      "\n",
      "M\n",
      ":\n",
      "2\n",
      "/\n",
      "4\n",
      "\n",
      "\n",
      "L\n",
      ":\n",
      "1\n",
      "/\n",
      "8\n",
      "\n",
      "\n",
      "K\n",
      ":\n",
      "D\n",
      "\n",
      "\n",
      "F\n",
      "D\n",
      " \n",
      "D\n",
      "E\n",
      "|\n",
      "G\n",
      "B\n",
      " \n",
      "d\n",
      "/\n",
      "d\n",
      "/\n",
      "c\n",
      "/\n",
      "B\n",
      "/\n",
      "|\n",
      "A\n",
      "B\n",
      " \n",
      "A\n",
      "/\n",
      "B\n",
      "/\n",
      "d\n",
      "/\n",
      "e\n",
      "/\n",
      "|\n",
      "b\n",
      "a\n",
      " \n",
      "a\n",
      "/\n",
      "g\n",
      "/\n",
      "e\n",
      "/\n",
      "d\n",
      "/\n",
      "|\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "/\n",
      "b\n",
      "/\n",
      "b\n",
      "/\n",
      "a\n",
      "/\n",
      "|\n",
      "b\n",
      "a\n",
      " \n",
      "g\n",
      "/\n",
      "$\n",
      "3\n",
      "/\n",
      "a\n",
      "/\n",
      "g\n",
      "/\n",
      "|\n",
      "b\n",
      "a\n",
      " \n",
      "f\n",
      "a\n",
      "|\n",
      "e\n",
      "/\n",
      "d\n",
      "/\n",
      "f\n",
      "/\n",
      "e\n",
      "/\n",
      " \n",
      "d\n",
      "f\n",
      "|\n",
      "e\n",
      "/\n",
      "e\n",
      "/\n",
      "d\n",
      "/\n",
      "e\n",
      "/\n",
      " \n",
      "d\n",
      "2\n",
      "|\n",
      "c\n",
      "A\n",
      " \n",
      "c\n",
      "/\n",
      "c\n",
      "/\n",
      "d\n",
      "/\n",
      "f\n",
      "/\n",
      "|\n",
      "A\n",
      "2\n",
      ":\n",
      "|\n",
      "\n",
      "\n",
      "d\n",
      "e\n",
      " \n",
      "$\n",
      "g\n",
      "/\n",
      "e\n",
      "/\n",
      "g\n",
      "/\n",
      "f\n",
      "/\n",
      "|\n",
      "g\n",
      "/\n",
      "a\n",
      "/\n",
      "g\n",
      "/\n",
      "g\n",
      "/\n",
      " \n",
      "f\n",
      "/\n",
      "e\n",
      "/\n",
      "e\n",
      "|\n",
      "d\n",
      "/\n",
      "e\n",
      "/\n",
      "f\n",
      "/\n",
      "B\n",
      "/\n",
      " \n",
      "c\n",
      "d\n",
      "|\n",
      "1\n",
      " \n",
      "e\n",
      "d\n",
      "/\n",
      "c\n",
      "/\n",
      " \n",
      "d\n",
      "2\n",
      ":\n",
      "|\n",
      "2\n",
      " \n",
      "G\n",
      "e\n",
      " \n",
      "d\n",
      ">\n",
      "f\n",
      "|\n",
      "|\n",
      "\n",
      "\n",
      "|\n",
      ":\n",
      "a\n",
      ">\n",
      "g\n",
      " \n",
      "e\n",
      "2\n",
      "|\n",
      "f\n",
      "e\n",
      " \n",
      "f\n",
      "e\n",
      "|\n",
      "a\n",
      " \n",
      "|\n",
      " \n",
      "f\n",
      "/\n",
      "f\n",
      "/\n",
      "f\n",
      "/\n",
      "f\n",
      "/\n",
      " \n",
      "e\n",
      "f\n",
      "/\n",
      "f\n",
      "/\n",
      " \n",
      "|\n",
      " \n",
      "e\n",
      "d\n",
      " \n",
      "d\n",
      "/\n",
      "e\n",
      "/\n",
      "d\n",
      "/\n",
      "c\n",
      "/\n",
      " \n",
      "|\n",
      " \n",
      "g\n",
      "f\n",
      " \n",
      "^\n",
      "g\n",
      "f\n",
      " \n",
      "|\n",
      "1\n",
      " \n",
      "d\n",
      ">\n",
      "d\n",
      " \n",
      "e\n",
      ">\n",
      "d\n",
      " \n",
      ":\n",
      "|\n",
      "2\n",
      " \n",
      "f\n",
      "e\n",
      "/\n",
      "d\n",
      "/\n",
      " \n",
      "^\n",
      "c\n",
      "2\n",
      " \n",
      "|\n",
      "]\n",
      "\n",
      "\n",
      "%\n",
      "\n",
      "\n",
      "%\n",
      "\n",
      "\n",
      "<\n",
      ":\n",
      "G\n",
      "2\n",
      "c\n",
      "B\n",
      "B\n",
      "=\n",
      "c\n",
      "2\n",
      " \n",
      "|\n",
      " \n",
      "$\n",
      "e\n",
      "c\n",
      "A\n",
      ")\n",
      "A\n",
      "G\n",
      " \n",
      "c\n",
      "d\n",
      "B\n",
      "c\n",
      " \n",
      "|\n",
      " \n",
      "d\n",
      "4\n",
      " \n",
      "f\n",
      "g\n",
      " \n",
      "f\n",
      "d\n",
      " \n",
      "A\n",
      "2\n",
      " \n",
      ":\n",
      "|\n",
      "\n",
      "\n",
      "T\n",
      ":\n",
      "K\n",
      "e\n",
      "r\n",
      "i\n",
      "l\n",
      "i\n",
      "?\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      " \n",
      "G\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "v\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "?\n",
      "e\n",
      "\n",
      "\n",
      "~\n",
      ":\n",
      "3\n",
      "2\n",
      "\n",
      "\n",
      "W\n",
      ":\n",
      "I\n",
      "s\n",
      " \n",
      "t\n",
      "i\n",
      "-\n",
      "b\n",
      "h\n",
      "a\n",
      "l\n",
      " \n",
      "M\n",
      "e\n",
      "c\n",
      "e\n",
      "-\n",
      "l\n",
      "e\n",
      "f\n",
      "t\n",
      "r\n",
      "e\n",
      "n\n",
      "a\n",
      "i\n",
      "l\n",
      " \n",
      "t\n",
      "o\n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "l\n",
      "e\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      " \n",
      "A\n",
      "r\n",
      "?\n",
      "g\n",
      "u\n",
      "a\n",
      "u\n",
      "\n",
      "\n",
      "%\n",
      "\n",
      "\n",
      "W\n",
      ":\n",
      "5\n",
      "\n",
      "\n",
      "T\n",
      ":\n",
      "M\n",
      "i\n",
      "s\n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      ",\n",
      " \n",
      "-\n",
      " \n",
      "J\n",
      "a\n",
      "n\n",
      "\n",
      "\n",
      "R\n",
      ":\n",
      "\"\n",
      "l\n",
      "o\n",
      "o\n",
      "n\n",
      ":\n",
      "\"\n",
      "F\n",
      "n\n",
      "\\\n",
      "'\n",
      "i\n",
      " \n",
      "H\n",
      "o\n",
      "r\n",
      "n\n",
      "p\n",
      "i\n",
      "p\n",
      "e\n",
      " \n",
      "$\n",
      "2\n",
      "9\n",
      "0\n",
      "0\n",
      "\n",
      "\n",
      "D\n",
      ":\n",
      "M\n",
      "a\n",
      "r\n",
      "y\n",
      " \n",
      "G\n",
      "y\n",
      "l\n",
      "a\n",
      "g\n",
      "h\n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "6\n",
      " \n",
      "1\n",
      "6\n",
      "7\n",
      "0\n",
      "1\n",
      "s\n",
      "2\n",
      "g\n",
      "\n",
      "\n",
      "Z\n",
      ":\n",
      "i\n",
      "d\n",
      ":\n",
      "h\n",
      "n\n",
      "-\n",
      "p\n",
      "o\n",
      "l\n",
      "k\n",
      "a\n",
      "-\n",
      "4\n",
      "5\n",
      "\n",
      "\n",
      "M\n",
      ":\n",
      "C\n",
      "|\n",
      "\n",
      "\n",
      "L\n",
      ":\n",
      "1\n",
      "/\n",
      "4\n",
      "\n",
      "\n",
      "K\n",
      ":\n",
      "D\n",
      "\n",
      "\n",
      "A\n",
      "A\n",
      " \n",
      "e\n",
      "f\n",
      "g\n",
      "|\n",
      "e\n",
      "d\n",
      "B\n",
      " \n",
      "d\n",
      "d\n",
      "2\n",
      "|\n",
      "f\n",
      "d\n",
      "/\n",
      "c\n",
      "/\n",
      " \n",
      "d\n",
      "B\n",
      "A\n",
      "G\n",
      "|\n",
      "A\n",
      "c\n",
      "e\n",
      "f\n",
      " \n",
      "e\n",
      "d\n",
      "B\n",
      "|\n",
      "c\n",
      "A\n",
      " \n",
      "G\n",
      "/\n",
      "F\n",
      "/\n",
      "F\n",
      "/\n",
      "A\n",
      "/\n",
      " \n",
      "B\n",
      "2\n",
      "|\n",
      "A\n",
      "G\n",
      " \n",
      "G\n",
      "F\n",
      "G\n",
      "G\n",
      "|\n",
      "\n",
      "\n",
      "%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X\n",
      ":\n",
      "3\n",
      "7\n",
      "\n",
      "\n",
      "T\n",
      ":\n",
      "L\n",
      "e\n",
      " \n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      "s\n",
      "p\n",
      ")\n",
      " \n",
      "n\n",
      "'\n",
      "e\n",
      " \n",
      "S\n",
      "t\n",
      "r\n",
      "o\n",
      "l\n",
      "y\n",
      "a\n",
      "s\n",
      " \n",
      "T\n",
      "h\n",
      "e\n",
      "\n",
      "\n",
      "T\n",
      ":\n",
      "P\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "Q\n",
      "l\n",
      "e\n",
      "i\n",
      "s\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      "r\n",
      "e\n",
      "m\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "w\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "P\n",
      "i\n",
      "e\n",
      "t\n",
      " \n",
      "P\n",
      "r\n",
      "o\n",
      "t\n",
      "t\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "H\n",
      "o\n",
      "r\n",
      "n\n",
      "p\n",
      "i\n",
      "p\n",
      "e\n",
      "\n",
      "\n",
      "T\n",
      ":\n",
      "P\n",
      "l\n",
      "a\n",
      "y\n",
      "k\n",
      " \n",
      "B\n",
      "a\n",
      "n\n",
      "f\n",
      "y\n",
      "\n",
      "\n",
      "M\n",
      ":\n",
      "3\n",
      "/\n",
      "8\n",
      "\n",
      "\n",
      "K\n",
      ":\n",
      "G\n",
      "\n",
      "\n",
      "G\n",
      "A\n",
      " \n",
      "B\n",
      "d\n",
      "/\n",
      "B\n",
      "/\n",
      "|\n",
      "B\n",
      "c\n",
      "/\n",
      "B\n",
      "/\n",
      " \n",
      "B\n",
      "A\n",
      "|\n",
      "B\n",
      "2\n",
      " \n",
      "G\n",
      "2\n",
      "|\n",
      "B\n",
      "A\n",
      " \n",
      "G\n",
      "F\n",
      "|\n",
      "A\n",
      "/\n",
      "B\n",
      "/\n",
      "A\n",
      "F\n",
      "G\n",
      " \n",
      "A\n",
      "B\n",
      "|\n",
      "A\n",
      "D\n",
      " \n",
      "F\n",
      "A\n",
      "/\n",
      "c\n",
      "/\n",
      "d\n",
      "|\n",
      "c\n",
      "d\n",
      " \n",
      "c\n",
      "e\n",
      "|\n",
      "f\n",
      "e\n",
      " \n",
      "d\n",
      "/\n",
      "B\n",
      "/\n",
      "A\n",
      "/\n",
      "B\n",
      "/\n",
      "|\n",
      "G\n",
      "3\n",
      " \n",
      "^\n",
      "F\n",
      "G\n",
      "|\n",
      "G\n",
      "A\n",
      " \n",
      "A\n",
      "/\n",
      "G\n",
      "/\n",
      "G\n",
      "/\n",
      "A\n",
      "/\n",
      "|\n",
      "d\n",
      "3\n",
      " \n",
      "e\n",
      "d\n",
      "|\n",
      "d\n",
      "c\n",
      " \n",
      "d\n",
      "e\n",
      "|\n",
      "g\n",
      "g\n",
      " \n",
      "B\n",
      "G\n",
      ":\n",
      "|\n",
      "\n",
      "\n",
      "|\n",
      ":\n",
      "c\n",
      "a\n",
      "/\n",
      "d\n",
      "/\n",
      " \n",
      "c\n",
      "e\n",
      "|\n",
      "c\n",
      "c\n",
      " \n",
      "c\n",
      "a\n",
      "/\n",
      "g\n",
      "/\n",
      "|\n",
      "B\n",
      "c\n",
      " \n",
      "e\n",
      "/\n",
      "g\n",
      "/\n",
      "b\n",
      "/\n",
      "a\n",
      "/\n",
      "|\n",
      "b\n",
      "a\n",
      " \n",
      "e\n",
      "c\n",
      "|\n",
      "d\n",
      "2\n",
      " \n",
      "A\n",
      "G\n",
      "/\n",
      "c\n",
      "/\n",
      ":\n",
      "|\n",
      "\n",
      "\n",
      "%\n"
     ]
    }
   ],
   "source": [
    "#get first initial input\n",
    "output = torch.from_numpy(np.array(one_hot_dict_encode['$'])).float()[None,None,:].to(computing_device) \n",
    "                \n",
    "#init hidden state\n",
    "hidden_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "hidden_state = hidden_state.float()\n",
    "cell_state = torch.zeros((config['num_layers'], 1, config['num_neurons'])).to(computing_device)\n",
    "cell_state = cell_state.float()\n",
    "hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "new_song = []\n",
    "\n",
    "#generate music\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    while output.squeeze().argmax() != 94 or len(new_song) <= config['max_song_length']: \n",
    "        output, hidden = model(output, hidden)\n",
    "        \n",
    "        softmax = F.softmax(output/config['temperature'])\n",
    "        \n",
    "        start = 0\n",
    "        partitions = []\n",
    "        \n",
    "        for i in softmax[0]:\n",
    "            partitions.append((start, start + i.item()))\n",
    "            start = start + i.item()\n",
    "        \n",
    "        roll = random.uniform(0, partitions[-1][1])\n",
    "        \n",
    "        \n",
    "        guess = 0\n",
    "        for partition in partitions:\n",
    "            if roll >= partition[0] and roll < partition[1]:\n",
    "                break\n",
    "            else : \n",
    "                guess += 1\n",
    "        \n",
    "        print(len(new_song))\n",
    "\n",
    "        next_input = np.zeros(config['input_dim'])\n",
    "        next_input[guess] = 1\n",
    "        \n",
    "        new_song.append(next_input)\n",
    "        \n",
    "        output = torch.from_numpy(next_input).float()[None, None, :].to(computing_device)\n",
    "                \n",
    "        \n",
    "for encoding in new_song:\n",
    "    print(one_hot_dict_decode[tuple(encoding)])\n",
    "            \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
